{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN 做文本生成\n",
    "用word 级别来做\n",
    "我们这里的文本预测就是，给了前面的单词以后，下一个单词是谁？\n",
    "比如，hello from the other, 给出 side\n",
    "\n",
    "英文预料：古登堡计划网站下载txt平文本：https://www.gutenberg.org/wiki/Category:Bookshelf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一步，先导入各种库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nltk\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读入文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77294\n",
      "[['\\ufeff', 'twenty', 'years', 'after', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'united', 'states', 'and', 'most', 'other', 'parts', 'of', 'the', 'world', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', '.'], ['you', 'may', 'copy', 'it', ',', 'give', 'it', 'away', 'or', 're-use', 'it', 'under', 'the', 'terms', 'of', 'the', 'project', 'gutenberg', 'license', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'http', ':', '//www.gutenberg.org/license', '.'], ['if', 'you', 'are', 'not', 'located', 'in', 'the', 'united', 'states', ',', 'you', '’', 'll', 'have', 'to', 'check', 'the', 'laws', 'of', 'the', 'country', 'where', 'you', 'are', 'located', 'before', 'using', 'this', 'ebook', '.']]\n"
     ]
    }
   ],
   "source": [
    "raw_text = ''\n",
    "for file in os.listdir(\"input/\"):\n",
    "    if file.endswith(\".txt\"):\n",
    "        raw_text += open(\"input/\"+file,encoding='utf-8', errors='ignore').read() + '\\n\\n'\n",
    "# raw_text = open('../input/Winston_Churchil.txt').read()\n",
    "raw_text = raw_text.lower() # 都变成小写\n",
    "sentensor = nltk.data.load('tokenizers/punkt/english.pickle')   # 分句\n",
    "sents = sentensor.tokenize(raw_text) # 分词\n",
    "corpus = []\n",
    "for sen in sents:\n",
    "    corpus.append(nltk.word_tokenize(sen))\n",
    "\n",
    "print(len(corpus))\n",
    "print(corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用word2vec 训练词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w2v_model=Word2Vec(corpus,size=128,window=5,min_count=5,workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -1.70100689e-01,   2.21460104e-01,  -5.34742922e-02,\n",
       "        -3.39673340e-01,   1.77219138e-01,  -2.05686405e-01,\n",
       "         1.34869337e-01,  -7.68376887e-02,  -1.71628296e-01,\n",
       "         5.59415556e-02,   5.84180467e-02,   7.74969533e-02,\n",
       "         8.75741690e-02,  -1.79418430e-01,   4.28613037e-01,\n",
       "         8.92448053e-02,  -1.04237899e-01,  -2.80936301e-01,\n",
       "         2.15246782e-01,   2.99152374e-01,  -1.17879227e-01,\n",
       "        -4.94978800e-02,   9.56530049e-02,  -1.87774166e-01,\n",
       "        -3.28286648e-01,   4.13469851e-01,   2.80880779e-01,\n",
       "         4.50759262e-01,  -3.73906106e-01,  -1.05421439e-01,\n",
       "        -3.49848598e-01,   1.53443813e-01,  -3.10900748e-01,\n",
       "         2.04842001e-01,   7.11158633e-01,  -4.84331548e-01,\n",
       "        -1.69602007e-01,  -2.75364816e-01,   9.72375646e-02,\n",
       "         1.91466376e-01,   4.30587023e-01,   3.45847696e-01,\n",
       "        -6.38228143e-03,   3.97485465e-01,   1.51778966e-01,\n",
       "        -5.76866388e-01,  -7.60715008e-02,  -1.20294556e-01,\n",
       "        -1.48702627e-02,  -1.28723159e-01,  -8.65817368e-02,\n",
       "        -4.49028939e-01,  -2.53689438e-01,   4.33397263e-01,\n",
       "         4.44452502e-02,  -1.22732870e-01,   9.37152952e-02,\n",
       "         8.42399821e-02,   4.22695249e-01,  -1.75572947e-01,\n",
       "        -4.91190106e-02,   1.32767901e-01,  -4.75955248e-01,\n",
       "         2.40609542e-01,  -2.52515405e-01,   1.03368880e-02,\n",
       "         8.98530483e-01,   8.11497495e-02,   4.04899061e-01,\n",
       "         3.64624619e-01,  -5.88045120e-01,  -8.55269194e-01,\n",
       "         2.47239277e-01,   2.82765120e-01,   3.86986554e-01,\n",
       "         3.13834637e-01,  -5.25720604e-02,  -3.46765488e-01,\n",
       "        -5.05653501e-01,  -3.65604669e-01,   1.47303820e-01,\n",
       "        -2.47062504e-01,   6.79235339e-01,   3.31963122e-01,\n",
       "         2.47853279e-01,   5.88151626e-02,  -1.09353036e-01,\n",
       "        -7.66508222e-01,   3.18996273e-02,   6.45207837e-02,\n",
       "         7.66257942e-02,   1.58579364e-01,  -3.38966280e-01,\n",
       "         1.18033566e-01,   3.47300954e-02,   4.97904837e-01,\n",
       "        -4.85562533e-01,  -2.17272103e-01,  -2.07149029e-01,\n",
       "        -5.75667620e-03,   5.35817683e-01,  -1.15946226e-01,\n",
       "         2.59717643e-01,  -1.67788714e-01,   1.17364135e-02,\n",
       "        -1.17947400e-01,  -1.99995056e-01,   1.51919171e-01,\n",
       "        -4.92901415e-01,   5.07445216e-01,  -1.59093335e-01,\n",
       "         2.81856269e-01,  -6.17922097e-02,   4.26540226e-01,\n",
       "        -3.62915656e-04,   2.62439668e-01,  -2.91319728e-01,\n",
       "         3.67395967e-01,  -9.44371596e-02,  -1.15789928e-01,\n",
       "        -4.37253833e-01,   1.28062502e-01,   1.36966957e-02,\n",
       "         4.54051793e-01,  -1.16843291e-01,   2.23558381e-01,\n",
       "        -1.01111777e-01,   1.87570438e-01], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model['office']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(w2v_model['office'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model['office'].shape # 每个单词为128维向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "处理我们的training data，把源数据变成一个长长的x，好让LSTM学会predict下一个单词："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1674820"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_input=[item for sublist in corpus for item in sublist]\n",
    "len(raw_input) # 一共这么多个单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'anywhere'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_input[12] # 比如看看第13个单词是啥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1610628"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 由于word2vec 训练时忽略掉了次数小于5的单词，这些词没有词向量，所以过滤掉这些词\n",
    "text_stream=[]\n",
    "vocab = w2v_model.wv.vocab  # w2v_model.vocab\n",
    "for word in raw_input:\n",
    "    if word in vocab:\n",
    "        text_stream.append(word)\n",
    "len(text_stream) # 过滤掉之后剩下的单词数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构造训练测试集\n",
    "将 text_stream 变成可以用来训练的x,y:\n",
    "x 是前 seq_length个字母，y 是后一个字母"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_length=10\n",
    "x=[]\n",
    "y=[]\n",
    "for i in range(0,len(text_stream)-seq_length):\n",
    "    given=text_stream[i:i+seq_length]\n",
    "    predict=text_stream[i+seq_length]\n",
    "    x.append( np.array([ w2v_model[word] for word in given]) ) #\n",
    "    y.append( w2v_model[predict] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0315582   0.24742642  0.29473412 ..., -0.10257404 -0.24917811\n",
      "   0.1610685 ]\n",
      " [ 0.01192663  0.12200874  0.23699829 ...,  0.09416112 -0.04142332\n",
      "   0.23037018]\n",
      " [ 0.06315383  0.59978247  0.65551829 ..., -0.1895072   0.32071814\n",
      "  -0.44224665]\n",
      " ..., \n",
      " [-0.86598939  1.40320039  0.72081333 ..., -0.00419424  0.12443067\n",
      "   0.38223574]\n",
      " [ 0.59775227 -0.67513627 -0.44468141 ...,  0.62417305 -0.38476896\n",
      "   0.84131485]\n",
      " [ 0.10815871  0.05645576 -0.19100019 ...,  0.10955054 -0.24792205\n",
      "   0.42896098]]\n",
      "[  5.28161407e-01   3.88861239e-01  -6.23571336e-01   1.45045757e+00\n",
      "   2.54875493e+00  -4.13271219e-01   2.00851753e-01   2.93023251e-02\n",
      "  -9.66639742e-02  -9.68550205e-01   1.98360360e+00  -1.40464497e+00\n",
      "   2.74905294e-01  -2.74265409e-01  -1.01635468e+00  -3.77958789e-02\n",
      "  -8.65953445e-01  -1.06454480e+00   1.03226149e+00   4.96265352e-01\n",
      "  -6.67271018e-01   1.20532477e+00  -8.60495925e-01  -3.98858190e-01\n",
      "   1.30271006e+00   9.68776122e-02   4.86399949e-01   2.51258820e-01\n",
      "  -9.58744109e-01  -3.76411378e-01  -6.94750547e-01  -2.77918816e-01\n",
      "  -8.95464599e-01   6.63579345e-01   1.15156658e-02  -1.89184114e-01\n",
      "   1.08133638e+00   5.44196427e-01  -1.52961397e+00  -2.39580929e-01\n",
      "   1.21585810e+00   1.51856995e+00  -2.69288838e-01   5.16087770e-01\n",
      "   4.93690193e-01  -7.63305545e-01  -4.63998109e-01   6.14028394e-01\n",
      "   8.76049578e-01   1.82485220e-03  -9.41311598e-01  -3.96988660e-01\n",
      "  -1.21052274e-02  -1.01817679e+00  -2.56428885e+00   2.37465277e-01\n",
      "  -1.59276947e-01   1.59611940e+00   5.07787585e-01   3.15918624e-01\n",
      "  -3.41254264e-01  -3.45001131e-01  -1.13433170e+00  -9.57808137e-01\n",
      "  -1.03749311e+00  -3.40001255e-01   1.01916456e+00  -1.50637817e+00\n",
      "   4.02933806e-01   1.14363790e+00   8.08052838e-01  -9.20358658e-01\n",
      "   1.07230641e-01  -2.36742663e+00  -6.73318878e-02  -7.11637855e-01\n",
      "   9.07711804e-01  -1.50535300e-01   1.75070524e-01  -7.20021904e-01\n",
      "   4.73969132e-01  -5.55558741e-01   1.26382816e+00   1.93340445e+00\n",
      "   1.89180231e+00  -6.58529878e-01  -3.86912763e-01  -6.53871775e-01\n",
      "   1.61794797e-01   3.68707716e-01   4.17054534e-01   5.50065562e-02\n",
      "   6.81328237e-01  -3.46145630e-01  -1.98302254e-01  -9.68254328e-01\n",
      "  -1.06964457e+00  -2.30572462e-01  -1.17320538e-01  -6.40616596e-01\n",
      "  -5.85540235e-01   1.50867033e+00  -3.90907913e-01  -7.20515370e-01\n",
      "  -3.36737424e-01  -4.92336065e-01  -1.38947344e+00   5.57075115e-03\n",
      "  -4.96488214e-01   6.90510333e-01  -2.37061810e-02   4.76080298e-01\n",
      "  -1.15258372e+00  -3.55630100e-01   6.84832394e-01  -4.36210066e-01\n",
      "  -3.44158411e-01   5.75648546e-01  -2.26399407e-01   7.08285868e-01\n",
      "   7.94239461e-01  -1.20137513e+00  -9.29349482e-01  -1.72946602e-01\n",
      "   1.47069955e+00  -7.36720860e-01  -2.94439733e-01  -4.26586986e-01]\n"
     ]
    }
   ],
   "source": [
    "print(x[10])\n",
    "print(y[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(y[10])) # y 中每个元素都是numpy array\n",
    "print(type(x[10][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1610618\n",
      "1610618\n",
      "10\n",
      "128\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "print(len(x)) # 一共样本数\n",
    "print(len(y))\n",
    "print(len(x[12])) # 每个样本的单词向量数\n",
    "print(len(x[12][0])) # 每个单词向量的维数\n",
    "print(len(y[12])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "转换为LSTM需要的数据格式 [样本数，时间步伐，特征]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.reshape(x, (-1, seq_length, 128))\n",
    "y = np.reshape(y, (-1,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型构造\n",
    "LSTM模型构造"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(28, input_shape=(10, 128), dropout=0.2, recurrent_dropout=0.2)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "model.add(LSTM(28, dropout_W=0.2, dropout_U=0.2, input_shape=(seq_length, 128))) #28是神经元个数,用256效果更好\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128,activation='sigmoid'))\n",
    "model.compile(loss='mse',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\models.py:848: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1610618/1610618 [==============================] - 524s - loss: 0.6608   \n",
      "Epoch 2/50\n",
      "1610618/1610618 [==============================] - 173s - loss: 0.6364   \n",
      "Epoch 3/50\n",
      "1610618/1610618 [==============================] - 177s - loss: 0.6302   \n",
      "Epoch 4/50\n",
      "1610618/1610618 [==============================] - 178s - loss: 0.6263   \n",
      "Epoch 5/50\n",
      "1610618/1610618 [==============================] - 181s - loss: 0.6235   \n",
      "Epoch 6/50\n",
      "1610618/1610618 [==============================] - 178s - loss: 0.6208   \n",
      "Epoch 7/50\n",
      "1610618/1610618 [==============================] - 183s - loss: 0.6188   \n",
      "Epoch 8/50\n",
      "1610618/1610618 [==============================] - 179s - loss: 0.6170   \n",
      "Epoch 9/50\n",
      "1610618/1610618 [==============================] - 176s - loss: 0.6156   \n",
      "Epoch 10/50\n",
      "1610618/1610618 [==============================] - 176s - loss: 0.6146   \n",
      "Epoch 11/50\n",
      "1610618/1610618 [==============================] - 175s - loss: 0.6136   \n",
      "Epoch 12/50\n",
      "1610618/1610618 [==============================] - 176s - loss: 0.6127   \n",
      "Epoch 13/50\n",
      "1610618/1610618 [==============================] - 176s - loss: 0.6120   \n",
      "Epoch 14/50\n",
      "1610618/1610618 [==============================] - 179s - loss: 0.6114   \n",
      "Epoch 15/50\n",
      "1610618/1610618 [==============================] - 175s - loss: 0.6109   \n",
      "Epoch 16/50\n",
      "1610618/1610618 [==============================] - 181s - loss: 0.6105   \n",
      "Epoch 17/50\n",
      "1610618/1610618 [==============================] - 176s - loss: 0.6101   \n",
      "Epoch 18/50\n",
      "1610618/1610618 [==============================] - 175s - loss: 0.6097   \n",
      "Epoch 19/50\n",
      "1610618/1610618 [==============================] - 176s - loss: 0.6095   \n",
      "Epoch 20/50\n",
      "1610618/1610618 [==============================] - 177s - loss: 0.6093   \n",
      "Epoch 21/50\n",
      "1610618/1610618 [==============================] - 172s - loss: 0.6090   \n",
      "Epoch 22/50\n",
      "1610618/1610618 [==============================] - 174s - loss: 0.6088   \n",
      "Epoch 23/50\n",
      "1610618/1610618 [==============================] - 175s - loss: 0.6086   \n",
      "Epoch 24/50\n",
      "1610618/1610618 [==============================] - 174s - loss: 0.6084   \n",
      "Epoch 25/50\n",
      "1610618/1610618 [==============================] - 174s - loss: 0.6083   \n",
      "Epoch 26/50\n",
      "1610618/1610618 [==============================] - 173s - loss: 0.6081   \n",
      "Epoch 27/50\n",
      "1610618/1610618 [==============================] - 173s - loss: 0.6080   \n",
      "Epoch 28/50\n",
      "1610618/1610618 [==============================] - 174s - loss: 0.6078   \n",
      "Epoch 29/50\n",
      "1610618/1610618 [==============================] - 174s - loss: 0.6077   \n",
      "Epoch 30/50\n",
      "1610618/1610618 [==============================] - 172s - loss: 0.6076   \n",
      "Epoch 31/50\n",
      "1610618/1610618 [==============================] - 173s - loss: 0.6075   \n",
      "Epoch 32/50\n",
      "1610618/1610618 [==============================] - 173s - loss: 0.6075   \n",
      "Epoch 33/50\n",
      "1610618/1610618 [==============================] - 173s - loss: 0.6073   \n",
      "Epoch 34/50\n",
      "1610618/1610618 [==============================] - 174s - loss: 0.6073   \n",
      "Epoch 35/50\n",
      "1610618/1610618 [==============================] - 173s - loss: 0.6073   \n",
      "Epoch 36/50\n",
      "1610618/1610618 [==============================] - 176s - loss: 0.6072   \n",
      "Epoch 37/50\n",
      "1610618/1610618 [==============================] - 175s - loss: 0.6071   \n",
      "Epoch 38/50\n",
      "1610618/1610618 [==============================] - 188s - loss: 0.6070   \n",
      "Epoch 39/50\n",
      "1610618/1610618 [==============================] - 180s - loss: 0.6070   \n",
      "Epoch 40/50\n",
      "1610618/1610618 [==============================] - 189s - loss: 0.6069   \n",
      "Epoch 41/50\n",
      "1610618/1610618 [==============================] - 190s - loss: 0.6069   \n",
      "Epoch 42/50\n",
      "1610618/1610618 [==============================] - 188s - loss: 0.6068   \n",
      "Epoch 43/50\n",
      "1610618/1610618 [==============================] - 191s - loss: 0.6068   \n",
      "Epoch 44/50\n",
      "1610618/1610618 [==============================] - 193s - loss: 0.6067   \n",
      "Epoch 45/50\n",
      "1610618/1610618 [==============================] - 187s - loss: 0.6067   \n",
      "Epoch 46/50\n",
      "1610618/1610618 [==============================] - 180s - loss: 0.6066   \n",
      "Epoch 47/50\n",
      "1610618/1610618 [==============================] - 179s - loss: 0.6066   \n",
      "Epoch 48/50\n",
      "1610618/1610618 [==============================] - 177s - loss: 0.6065   \n",
      "Epoch 49/50\n",
      "1610618/1610618 [==============================] - 168s - loss: 0.6065   \n",
      "Epoch 50/50\n",
      "1610618/1610618 [==============================] - 174s - loss: 0.6065   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c7f5274320>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x,y,nb_epoch=50,batch_size=2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 效果检验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_next(input_array):\n",
    "    x=np.reshape(input_array,(-1,seq_length,128))\n",
    "    y=model.predict(x)\n",
    "    return y\n",
    "\n",
    "def string_to_index(raw_input):\n",
    "    raw_input=raw_input.lower()\n",
    "    input_stream=nltk.word_tokenize(raw_input)\n",
    "    res=[]\n",
    "    for word in input_stream[(len(input_stream)-seq_length):]:\n",
    "        res.append(w2v_model[word])\n",
    "    return res\n",
    "\n",
    "def y_to_word(y):\n",
    "    word=w2v_model.most_similar(positive=y,topn=1)\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_article(init,rounds=30):\n",
    "    in_string=init.lower()\n",
    "    for i in range(rounds):\n",
    "        n=y_to_word(predict_next(string_to_index(in_string)))\n",
    "        in_string+=' '+n[0][0]\n",
    "    return in_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language models allow us to measure how likely a sentence is, which is an important for machine due due terrible terrible terrible terrible terrible terrible terrible terrible terrible terrible terrible terrible terrible terrible terrible terrible terrible terrible terrible terrible terrible terrible terrible terrible terrible terrible terrible terrible\n"
     ]
    }
   ],
   "source": [
    "init = 'Language Models allow us to measure how likely a sentence is, which is an important for Machine'\n",
    "article = generate_article(init)\n",
    "print(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
