{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n",
      "276830\n",
      "[[60, 45, 47, 44, 39, 34, 32, 49, 1, 36, 50, 49, 34, 43, 31, 34, 47, 36, 57, 48, 1, 47, 34, 30, 41, 1, 48, 44, 41, 33, 38, 34, 47, 48, 1, 44, 35, 1, 35, 44, 47, 49, 50, 43, 34, 9, 1, 31, 54, 1, 47, 38, 32, 37, 30, 47, 33, 1, 37, 30, 47, 33, 38, 43, 36, 1, 33, 30, 51, 38, 48, 0, 0, 49, 37, 38, 48, 1, 34, 31, 44, 44, 40, 1, 38, 48, 1, 35, 44, 47, 1, 49, 37, 34, 1, 50, 48, 34, 1, 44], [45, 47, 44, 39, 34, 32, 49, 1, 36, 50, 49, 34, 43, 31, 34, 47, 36, 57, 48, 1, 47, 34, 30, 41, 1, 48, 44, 41, 33, 38, 34, 47, 48, 1, 44, 35, 1, 35, 44, 47, 49, 50, 43, 34, 9, 1, 31, 54, 1, 47, 38, 32, 37, 30, 47, 33, 1, 37, 30, 47, 33, 38, 43, 36, 1, 33, 30, 51, 38, 48, 0, 0, 49, 37, 38, 48, 1, 34, 31, 44, 44, 40, 1, 38, 48, 1, 35, 44, 47, 1, 49, 37, 34, 1, 50, 48, 34, 1, 44, 35], [47, 44, 39, 34, 32, 49, 1, 36, 50, 49, 34, 43, 31, 34, 47, 36, 57, 48, 1, 47, 34, 30, 41, 1, 48, 44, 41, 33, 38, 34, 47, 48, 1, 44, 35, 1, 35, 44, 47, 49, 50, 43, 34, 9, 1, 31, 54, 1, 47, 38, 32, 37, 30, 47, 33, 1, 37, 30, 47, 33, 38, 43, 36, 1, 33, 30, 51, 38, 48, 0, 0, 49, 37, 38, 48, 1, 34, 31, 44, 44, 40, 1, 38, 48, 1, 35, 44, 47, 1, 49, 37, 34, 1, 50, 48, 34, 1, 44, 35, 1]]\n",
      "[35, 1, 30]\n",
      "[[ 0.80327869]\n",
      " [ 0.55737705]\n",
      " [ 0.70491803]\n",
      " [ 0.50819672]\n",
      " [ 0.55737705]\n",
      " [ 0.7704918 ]\n",
      " [ 0.59016393]\n",
      " [ 0.93442623]\n",
      " [ 0.78688525]\n",
      " [ 0.01639344]\n",
      " [ 0.7704918 ]\n",
      " [ 0.55737705]\n",
      " [ 0.49180328]\n",
      " [ 0.67213115]\n",
      " [ 0.01639344]\n",
      " [ 0.78688525]\n",
      " [ 0.72131148]\n",
      " [ 0.67213115]\n",
      " [ 0.54098361]\n",
      " [ 0.62295082]\n",
      " [ 0.55737705]\n",
      " [ 0.7704918 ]\n",
      " [ 0.78688525]\n",
      " [ 0.01639344]\n",
      " [ 0.72131148]\n",
      " [ 0.57377049]\n",
      " [ 0.01639344]\n",
      " [ 0.57377049]\n",
      " [ 0.72131148]\n",
      " [ 0.7704918 ]\n",
      " [ 0.80327869]\n",
      " [ 0.81967213]\n",
      " [ 0.70491803]\n",
      " [ 0.55737705]\n",
      " [ 0.14754098]\n",
      " [ 0.01639344]\n",
      " [ 0.50819672]\n",
      " [ 0.8852459 ]\n",
      " [ 0.01639344]\n",
      " [ 0.7704918 ]\n",
      " [ 0.62295082]\n",
      " [ 0.52459016]\n",
      " [ 0.60655738]\n",
      " [ 0.49180328]\n",
      " [ 0.7704918 ]\n",
      " [ 0.54098361]\n",
      " [ 0.01639344]\n",
      " [ 0.60655738]\n",
      " [ 0.49180328]\n",
      " [ 0.7704918 ]\n",
      " [ 0.54098361]\n",
      " [ 0.62295082]\n",
      " [ 0.70491803]\n",
      " [ 0.59016393]\n",
      " [ 0.01639344]\n",
      " [ 0.54098361]\n",
      " [ 0.49180328]\n",
      " [ 0.83606557]\n",
      " [ 0.62295082]\n",
      " [ 0.78688525]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.80327869]\n",
      " [ 0.60655738]\n",
      " [ 0.62295082]\n",
      " [ 0.78688525]\n",
      " [ 0.01639344]\n",
      " [ 0.55737705]\n",
      " [ 0.50819672]\n",
      " [ 0.72131148]\n",
      " [ 0.72131148]\n",
      " [ 0.6557377 ]\n",
      " [ 0.01639344]\n",
      " [ 0.62295082]\n",
      " [ 0.78688525]\n",
      " [ 0.01639344]\n",
      " [ 0.57377049]\n",
      " [ 0.72131148]\n",
      " [ 0.7704918 ]\n",
      " [ 0.01639344]\n",
      " [ 0.80327869]\n",
      " [ 0.60655738]\n",
      " [ 0.55737705]\n",
      " [ 0.01639344]\n",
      " [ 0.81967213]\n",
      " [ 0.78688525]\n",
      " [ 0.55737705]\n",
      " [ 0.01639344]\n",
      " [ 0.72131148]\n",
      " [ 0.57377049]\n",
      " [ 0.01639344]\n",
      " [ 0.49180328]\n",
      " [ 0.70491803]\n",
      " [ 0.8852459 ]\n",
      " [ 0.72131148]\n",
      " [ 0.70491803]\n",
      " [ 0.55737705]\n",
      " [ 0.01639344]\n",
      " [ 0.49180328]\n",
      " [ 0.70491803]]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  1.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense #每个层级\n",
    "from keras.layers import Dropout # 为了防止过拟合，忽略掉中间的一些神经元\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# 读入文本，用丘吉尔的人物传记作为学习预料\n",
    "raw_text = open('input/Winston_Churchil.txt', encoding='UTF-8').read()\n",
    "raw_text = raw_text.lower() # 都变成小写\n",
    "\n",
    "chars = sorted(list(set(raw_text)))  # 得到所有字符\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars)) # 从字符到数字的对照表\n",
    "in_to_char = dict((i, c) for i, c in enumerate(chars)) # 从数字到字符的对照表\n",
    "\n",
    "print(len(chars))  # 61 个字符\n",
    "print(len(raw_text))  # 276830 个字符\n",
    "\n",
    "# 构造训练测试集\n",
    "# 我们需要把我们的 raw_text 变成可以训练的 x,y\n",
    "# x是前置字母们  y是后一个字母\n",
    "seq_length = 100  # 一个 x 的长度，即根据前100个字符预测下一个字符\n",
    "x = []\n",
    "y = []\n",
    "for i in range(0, len(raw_text) - seq_length):\n",
    "    given = raw_text[i:i + seq_length]\n",
    "    predict = raw_text[i + seq_length]\n",
    "    x.append([char_to_int[char] for char in given])  # 将字符存为词袋中的编号\n",
    "    y.append(char_to_int[predict])\n",
    "\n",
    "print(x[:3])  # 查看前三条x\n",
    "print(y[:3])\n",
    "\n",
    "# 我们已经有了一个Input数字形式的表达,我们要把它变成LSTM需要的数组格式：[样本数，时间步伐，特征长度]\n",
    "#  样本数是X一共有多少，时间步伐是记忆的长度，此题为100，特征是一个一个字符，所以是1\n",
    "# 对于output,用 one-hot 做output 的预测可以给我们更好的效果，相对于直接预测一个准确的y数值的话。\n",
    "n_patterns = len(x)  # 训练集个数\n",
    "n_vocab = len(chars)  # 字符的个数\n",
    "\n",
    "# 把x变成LSTM需要的样子\n",
    "x = numpy.reshape(x, (n_patterns, seq_length, 1))  # [样本数，时间步伐，特征长度] 个人理解 ：变成了一个样本数*时间步伐数*特征 的三维矩阵，每一行是一个样本，每一行的每个字符是一个特征\n",
    "# 简单normal到0-1之间\n",
    "x = x / float(n_vocab)\n",
    "# output变成ont-hot\n",
    "y = np_utils.to_categorical(y)  # 变成 样本数*61 的矩阵，相应位置是1，表示是这个字符 ，其他位置都是0\n",
    "\n",
    "print(x[11])\n",
    "print(y[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzz/anaconda3/lib/python3.6/site-packages/keras/models.py:851: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/55\n",
      "276730/276730 [==============================] - 17s - loss: 3.9884    \n",
      "Epoch 2/55\n",
      "276730/276730 [==============================] - 15s - loss: 3.6401    \n",
      "Epoch 3/55\n",
      "276730/276730 [==============================] - 15s - loss: 3.3207    \n",
      "Epoch 4/55\n",
      "276730/276730 [==============================] - 15s - loss: 3.1687    \n",
      "Epoch 5/55\n",
      "276730/276730 [==============================] - 15s - loss: 3.1089    \n",
      "Epoch 6/55\n",
      "276730/276730 [==============================] - 15s - loss: 3.0823    \n",
      "Epoch 7/55\n",
      "276730/276730 [==============================] - 15s - loss: 3.0662    \n",
      "Epoch 8/55\n",
      "276730/276730 [==============================] - 15s - loss: 3.0570    \n",
      "Epoch 9/55\n",
      "276730/276730 [==============================] - 15s - loss: 3.0510    \n",
      "Epoch 10/55\n",
      "276730/276730 [==============================] - 15s - loss: 3.0459    \n",
      "Epoch 11/55\n",
      "276730/276730 [==============================] - 15s - loss: 3.0431    \n",
      "Epoch 12/55\n",
      "276730/276730 [==============================] - 15s - loss: 3.0400    \n",
      "Epoch 13/55\n",
      "276730/276730 [==============================] - 15s - loss: 3.0379    \n",
      "Epoch 14/55\n",
      "276730/276730 [==============================] - 15s - loss: 3.0356    \n",
      "Epoch 15/55\n",
      "276730/276730 [==============================] - 15s - loss: 3.0343    \n",
      "Epoch 16/55\n",
      "276730/276730 [==============================] - 15s - loss: 3.0324    \n",
      "Epoch 17/55\n",
      "276730/276730 [==============================] - 15s - loss: 3.0311    \n",
      "Epoch 18/55\n",
      "276730/276730 [==============================] - 15s - loss: 3.0297    \n",
      "Epoch 19/55\n",
      "276730/276730 [==============================] - 15s - loss: 3.0296    \n",
      "Epoch 20/55\n",
      "276730/276730 [==============================] - 15s - loss: 3.0281    \n",
      "Epoch 21/55\n",
      "276730/276730 [==============================] - 15s - loss: 3.0271    \n",
      "Epoch 22/55\n",
      "276730/276730 [==============================] - 15s - loss: 3.0261    \n",
      "Epoch 23/55\n",
      "276730/276730 [==============================] - 15s - loss: 3.0256    \n",
      "Epoch 24/55\n",
      "276730/276730 [==============================] - 15s - loss: 3.0249    \n",
      "Epoch 25/55\n",
      "276730/276730 [==============================] - 15s - loss: 3.0243    \n",
      "Epoch 26/55\n",
      "276730/276730 [==============================] - 15s - loss: 3.0234    \n",
      "Epoch 27/55\n",
      "276730/276730 [==============================] - 15s - loss: 3.0238    \n",
      "Epoch 28/55\n",
      "276730/276730 [==============================] - 15s - loss: 3.0224    \n",
      "Epoch 29/55\n",
      "276730/276730 [==============================] - 15s - loss: 3.0212    \n",
      "Epoch 30/55\n",
      "276730/276730 [==============================] - 15s - loss: 3.0207    \n",
      "Epoch 31/55\n",
      "276730/276730 [==============================] - 15s - loss: 3.0206    \n",
      "Epoch 32/55\n",
      "276730/276730 [==============================] - 15s - loss: 3.0196    \n",
      "Epoch 33/55\n",
      "276730/276730 [==============================] - 16s - loss: 3.0196    \n",
      "Epoch 34/55\n",
      "276730/276730 [==============================] - 15s - loss: 3.0184    \n",
      "Epoch 35/55\n",
      "276730/276730 [==============================] - 15s - loss: 3.0179    \n",
      "Epoch 36/55\n",
      "276730/276730 [==============================] - 15s - loss: 3.0157    \n",
      "Epoch 37/55\n",
      "276730/276730 [==============================] - 15s - loss: 3.0118    \n",
      "Epoch 38/55\n",
      "276730/276730 [==============================] - 15s - loss: 3.0070    \n",
      "Epoch 39/55\n",
      "276730/276730 [==============================] - 15s - loss: 3.0011    \n",
      "Epoch 40/55\n",
      "276730/276730 [==============================] - 15s - loss: 2.9947    \n",
      "Epoch 41/55\n",
      "276730/276730 [==============================] - 15s - loss: 2.9887    \n",
      "Epoch 42/55\n",
      "276730/276730 [==============================] - 15s - loss: 2.9829    \n",
      "Epoch 43/55\n",
      "276730/276730 [==============================] - 15s - loss: 2.9780    \n",
      "Epoch 44/55\n",
      "276730/276730 [==============================] - 15s - loss: 2.9727    \n",
      "Epoch 45/55\n",
      "276730/276730 [==============================] - 15s - loss: 2.9688    \n",
      "Epoch 46/55\n",
      "276730/276730 [==============================] - 15s - loss: 2.9662    \n",
      "Epoch 47/55\n",
      "276730/276730 [==============================] - 15s - loss: 2.9630    \n",
      "Epoch 48/55\n",
      "276730/276730 [==============================] - 15s - loss: 2.9615    \n",
      "Epoch 49/55\n",
      "276730/276730 [==============================] - 15s - loss: 2.9594    \n",
      "Epoch 50/55\n",
      "276730/276730 [==============================] - 15s - loss: 2.9577    \n",
      "Epoch 51/55\n",
      "276730/276730 [==============================] - 15s - loss: 2.9565    \n",
      "Epoch 52/55\n",
      "276730/276730 [==============================] - 15s - loss: 2.9547    \n",
      "Epoch 53/55\n",
      "276730/276730 [==============================] - 15s - loss: 2.9544    \n",
      "Epoch 54/55\n",
      "276730/276730 [==============================] - 15s - loss: 2.9523    \n",
      "Epoch 55/55\n",
      "276730/276730 [==============================] - 15s - loss: 2.9513    \n",
      "his object in coming to new york was to engage officers for that service. he came at an opportune moment t at at at at a aa a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a \n"
     ]
    }
   ],
   "source": [
    "# 构建模型\n",
    "model = Sequential()\n",
    "model.add(LSTM(5, input_shape=(x.shape[1], x.shape[2]))) # 256是层数，input_shape=(时间步伐，特征长度)\n",
    "model.add(Dropout(0.2)) # 随机遗忘掉20%的神经元，避免轻易的落入局部最优解\n",
    "model.add(Dense(y.shape[1], activation='softmax')) # Keras中的一个普通神经网络称为Dense，Dense(输出数组的长度，激活函数)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model.fit(x, y, nb_epoch=55, batch_size=4096) # 每4096个数据一起跑，跑50圈\n",
    "\n",
    "\n",
    "# 验证模型效果\n",
    "def predict_next(input_array):\n",
    "    x = numpy.reshape(input_array, (1, seq_length, 1)) #使用相同的方式，变成LSTM需要的数组格式\n",
    "    x = x / float(n_vocab) #归一化为0-1之间的数\n",
    "    y = model.predict(x)\n",
    "    return y\n",
    "\n",
    "def string_to_index(raw_input):\n",
    "    res=[]\n",
    "    for c in raw_input[(len(raw_input)-seq_length):]:\n",
    "        res.append(char_to_int[c])\n",
    "    return res\n",
    "\n",
    "def y_to_char(y):\n",
    "    largest_index=y.argmax()\n",
    "    c=in_to_char[largest_index]\n",
    "    return c\n",
    "\n",
    "def generate_article(init,rounds=200):\n",
    "    in_string=init.lower()\n",
    "    for i in range(rounds):\n",
    "        n=y_to_char(predict_next(string_to_index(in_string)))\n",
    "        in_string+=n\n",
    "    return in_string\n",
    "\n",
    "init='His object in coming to New York was to engage officers for that service. He came at an opportune moment'\n",
    "article=generate_article(init)\n",
    "print(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "304"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(article)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
