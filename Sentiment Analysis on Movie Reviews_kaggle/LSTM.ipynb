{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import SGD, Adam, Nadam, RMSprop\n",
    "from keras.models import Sequential,Model,load_model\n",
    "from keras.layers import Embedding,Conv1D,MaxPooling1D\n",
    "from keras.layers.core import Dense, Activation,Dropout ,Flatten\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import text_to_word_sequence,one_hot,Tokenizer\n",
    "from keras.constraints import maxnorm\n",
    "from keras.callbacks import ModelCheckpoint,TensorBoard, ReduceLROnPlateau,EarlyStopping\n",
    "from keras.applications import Xception\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed=7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_TrainingData(path):\n",
    "    D=pd.read_csv(path,sep='\\t',header=0)\n",
    "    feature_names=np.array(list(D.columns.values))\n",
    "    X_train=np.array(list(D['Phrase']))\n",
    "    Y_train=np.array(list(D['Sentiment']))\n",
    "    return X_train,Y_train,feature_names\n",
    "\n",
    "def load_TestingData(path):     #loads data , caluclate Mean & subtract it data, gets the COV. Matrix.\n",
    "    D = pd.read_csv(path, sep='\\t', header=0)\n",
    "    X_test=np.array(list(D['Phrase']))\n",
    "    X_test_PhraseID=np.array(list(D['PhraseId']))\n",
    "    return  X_test,X_test_PhraseID\n",
    "\n",
    "def shuffle_2(a, b): # Shuffles 2 arrays with the same order\n",
    "    s = np.arange(a.shape[0])\n",
    "    np.random.shuffle(s)\n",
    "    return a[s], b[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== Training data shapes ==============================\n",
      "X_train.shape is  (156060,)\n",
      "Y_train.shape is  (156060,)\n"
     ]
    }
   ],
   "source": [
    "X_train,Y_train,feature_names=load_TrainingData('train.tsv')\n",
    "X_test,X_test_PhraseID = load_TestingData('test.tsv')\n",
    "print ('============================== Training data shapes ==============================')\n",
    "print ('X_train.shape is ', X_train.shape)\n",
    "print ('Y_train.shape is ',Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 17781\n"
     ]
    }
   ],
   "source": [
    "Tokenizer = Tokenizer()\n",
    "Tokenizer.fit_on_texts(np.concatenate((X_train, X_test), axis=0))\n",
    "# Tokenizer.fit_on_texts(X_train)\n",
    "Tokenizer_vocab_size = len(Tokenizer.word_index) + 1\n",
    "print(\"Vocab size\",Tokenizer_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#masking\n",
    "num_test = 32000\n",
    "mask = range(num_test)\n",
    "\n",
    "Y_Val = Y_train[:num_test]\n",
    "Y_Val2 = Y_train[:num_test]\n",
    "X_Val = X_train[:num_test]\n",
    "\n",
    "\n",
    "X_train = X_train[num_test:]\n",
    "Y_train = Y_train[num_test:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxWordCount= 60\n",
    "maxDictionary_size=Tokenizer_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded_words = Tokenizer.texts_to_sequences(X_train)\n",
    "encoded_words2 = Tokenizer.texts_to_sequences(X_Val)\n",
    "encoded_words3 = Tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#padding all text to same size\n",
    "X_Train_encodedPadded_words = sequence.pad_sequences(encoded_words, maxlen=maxWordCount)\n",
    "X_Val_encodedPadded_words = sequence.pad_sequences(encoded_words2, maxlen=maxWordCount)\n",
    "X_test_encodedPadded_words = sequence.pad_sequences(encoded_words3, maxlen=maxWordCount)\n",
    "\n",
    "# One Hot Encoding\n",
    "Y_train = keras.utils.to_categorical(Y_train, 5)\n",
    "Y_Val   = keras.utils.to_categorical(Y_Val, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[   0,    0,    0, ...,  184, 8871,   55],\n",
       "        [   0,    0,    0, ..., 3758,    3, 2537],\n",
       "        [   0,    0,    0, ...,    1,   96, 1300],\n",
       "        ..., \n",
       "        [   0,    0,    0, ...,   32,    1,  276],\n",
       "        [   0,    0,    0, ..., 6577,    4, 5931],\n",
       "        [   0,    0,    0, ...,    0,    0, 7287]], dtype=int32),\n",
       " array([[ 0.,  1.,  0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  1.,  0.],\n",
       "        ..., \n",
       "        [ 0.,  1.,  0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  1.,  0.,  0.]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shuffling the traing Set\n",
    "shuffle_2(X_Train_encodedPadded_words,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features are ['PhraseId' 'SentenceId' 'Phrase' 'Sentiment']\n",
      "-------After extracting a validation set of 32000--------\n",
      "============================== Training data shapes ==============================\n",
      "X_train.shape is  (124060,)\n",
      "Y_train.shape is  (124060, 5)\n",
      "============================== Validation data shapes ==============================\n",
      "Y_Val.shape is  (32000, 5)\n",
      "X_Val.shape is  (32000,)\n",
      "============================== Test data shape ==============================\n",
      "X_test.shape is  (66292,)\n"
     ]
    }
   ],
   "source": [
    "print('features are',feature_names)\n",
    "print(\"-------After extracting a validation set of \"+str(num_test)+\"--------\")\n",
    "print ('============================== Training data shapes ==============================')\n",
    "print ('X_train.shape is ', X_train.shape)\n",
    "print ('Y_train.shape is ',Y_train.shape)\n",
    "print ('============================== Validation data shapes ==============================')\n",
    "print ('Y_Val.shape is ',Y_Val.shape)\n",
    "print ('X_Val.shape is ', X_Val.shape)\n",
    "print ('============================== Test data shape ==============================')\n",
    "print ('X_test.shape is ', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== After padding all text to same size of 60 ==============================\n",
      "============================== Training data shapes ==============================\n",
      "X_Train_encodedPadded_words.shape is  (124060, 60)\n",
      "Y_train.shape is  (124060, 5)\n",
      "============================== Validation data shapes ==============================\n",
      "X_Val_encodedPadded_words.shape is  (32000, 60)\n",
      "Y_Val.shape is  (32000, 5)\n",
      "============================== Test data shape ==============================\n",
      "X_test_encodedPadded_words.shape is  (66292, 60)\n"
     ]
    }
   ],
   "source": [
    "print ('============================== After padding all text to same size of '+ str(maxWordCount)+' ==============================')\n",
    "print ('============================== Training data shapes ==============================')\n",
    "print ('X_Train_encodedPadded_words.shape is ', X_Train_encodedPadded_words.shape)\n",
    "print ('Y_train.shape is ',Y_train.shape)\n",
    "print ('============================== Validation data shapes ==============================')\n",
    "print ('X_Val_encodedPadded_words.shape is ', X_Val_encodedPadded_words.shape)\n",
    "print ('Y_Val.shape is ',Y_Val.shape)\n",
    "print ('============================== Test data shape ==============================')\n",
    "print ('X_test_encodedPadded_words.shape is ', X_test_encodedPadded_words.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 60, 32)            568992    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               82432     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 668,581\n",
      "Trainable params: 668,581\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzz/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(128, activation=\"relu\", kernel_constraint=<keras.con...)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "model=Sequential()\n",
    "model.add(Embedding(maxDictionary_size,210,input_length=maxWordCount))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(128, activation='relu',W_constraint=maxnorm(1)))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate=0.0001\n",
    "epochs = 2\n",
    "batch_size = 256 #32\n",
    "sgd = SGD(lr=learning_rate, nesterov=True, momentum=0.7, decay=1e-4)\n",
    "Nadam = keras.optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Nadam, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tensorboard=keras.callbacks.TensorBoard(log_dir='./logs/log_25',histogram_freq=0,write_graph=True,write_images=False) #该回调函数是一个可视化的展示器\n",
    "checkpointer=ModelCheckpoint(filepath='./weights/weights_25.hdf5',verbose=1,save_best_only=True,monitor='val_loss') #该回调函数将在每个epoch后保存模型到filepath\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=0, verbose=1, mode='auto', cooldown=0, min_lr=1e-6) #当评价指标不在提升时，减少学习率\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=6, verbose=1) #当监测值不再改善时，该回调函数将中止训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================== Training =========================================\n",
      "Train on 124060 samples, validate on 32000 samples\n",
      "Epoch 1/2\n",
      "123904/124060 [============================>.] - ETA: 0s - loss: 1.0558 - acc: 0.5789Epoch 00000: val_loss improved from inf to 0.91740, saving model to ./weights/weights_25.hdf5\n",
      "124060/124060 [==============================] - 61s - loss: 1.0557 - acc: 0.5789 - val_loss: 0.9174 - val_acc: 0.6325\n",
      "Epoch 2/2\n",
      "123904/124060 [============================>.] - ETA: 0s - loss: 0.8091 - acc: 0.6669Epoch 00001: val_loss improved from 0.91740 to 0.90788, saving model to ./weights/weights_25.hdf5\n",
      "124060/124060 [==============================] - 60s - loss: 0.8091 - acc: 0.6668 - val_loss: 0.9079 - val_acc: 0.6403\n"
     ]
    }
   ],
   "source": [
    "print (\"=============================== Training =========================================\")\n",
    "history  = model.fit(X_Train_encodedPadded_words, Y_train, epochs = epochs, batch_size=batch_size, verbose=1,\n",
    "                    validation_data=(X_Val_encodedPadded_words, Y_Val), callbacks=[tensorboard, reduce_lr,checkpointer,earlyStopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================== Score =========================================\n",
      "Accuracy: 64.03%\n"
     ]
    }
   ],
   "source": [
    "print (\"=============================== Score =========================================\")\n",
    "scores = model.evaluate(X_Val_encodedPadded_words, Y_Val, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================== Predicting =========================================\n",
      "66048/66292 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "print (\"=============================== Predicting =========================================\")\n",
    "f = open('Submission.csv', 'w')\n",
    "f.write('PhraseId,Sentiment\\n')\n",
    "predicted_classes = model.predict_classes(X_test_encodedPadded_words, batch_size=batch_size, verbose=1)\n",
    "for i in range(0,X_test_PhraseID.shape[0]):\n",
    "    # pred =np.argmax(predictions[i])\n",
    "    f.write(str(X_test_PhraseID[i])+\",\"+str(predicted_classes[i])+'\\n')\n",
    "    # print predictions[i],\"=>\",pred\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tensorboard --logdir=./logs/log_25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
