{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import SGD, Adam, Nadam, RMSprop\n",
    "from keras.models import Sequential,Model,load_model\n",
    "from keras.layers import Embedding,Conv1D,MaxPooling1D\n",
    "from keras.layers.core import Dense, Activation,Dropout ,Flatten\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import text_to_word_sequence,one_hot,Tokenizer\n",
    "from keras.constraints import maxnorm\n",
    "from keras.callbacks import ModelCheckpoint,TensorBoard, ReduceLROnPlateau,EarlyStopping\n",
    "from keras.applications import Xception\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed=7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_TrainingData(path):\n",
    "    D=pd.read_csv(path,sep='\\t',header=0)\n",
    "    feature_names=np.array(list(D.columns.values))\n",
    "    X_train=np.array(list(D['Phrase']))\n",
    "    Y_train=np.array(list(D['Sentiment']))\n",
    "    return X_train,Y_train,feature_names\n",
    "\n",
    "def load_TestingData(path):     #loads data , caluclate Mean & subtract it data, gets the COV. Matrix.\n",
    "    D = pd.read_csv(path, sep='\\t', header=0)\n",
    "    X_test=np.array(list(D['Phrase']))\n",
    "    X_test_PhraseID=np.array(list(D['PhraseId']))\n",
    "    return  X_test,X_test_PhraseID\n",
    "\n",
    "def shuffle_2(a, b): # Shuffles 2 arrays with the same order\n",
    "    s = np.arange(a.shape[0])\n",
    "    np.random.shuffle(s)\n",
    "    return a[s], b[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== Training data shapes ==============================\n",
      "X_train.shape is  (156060,)\n",
      "Y_train.shape is  (156060,)\n"
     ]
    }
   ],
   "source": [
    "X_train,Y_train,feature_names=load_TrainingData('train.tsv')\n",
    "X_test,X_test_PhraseID = load_TestingData('test.tsv')\n",
    "print ('============================== Training data shapes ==============================')\n",
    "print ('X_train.shape is ', X_train.shape)\n",
    "print ('Y_train.shape is ',Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-processing train docs...\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import SnowballStemmer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['.', ',', '\"', \"'\", ':', ';', '(', ')', '[', ']', '{', '}'])\n",
    "stemmer = SnowballStemmer('english') #词干提取\n",
    "print(\"pre-processing train docs...\")\n",
    "processed_docs=[]\n",
    "docs=np.concatenate((X_train, X_test), axis=0)\n",
    "for doc in docs:\n",
    "    tokens=word_tokenize(doc)\n",
    "    filtered=[word for word in tokens if word not in stop_words]\n",
    "    stemmed=[stemmer.stem(word) for word in filtered]\n",
    "    processed_docs.append(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222352"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "dictionary=corpora.Dictionary(processed_docs) #为语料库中出现的每个单词分配一个编号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary[0] #gensimd 的Dictionary类型是根据编号能得到单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary size:  13759\n"
     ]
    }
   ],
   "source": [
    "dictionary_size = len(dictionary.keys())\n",
    "print(\"dictionary size: \", dictionary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting to token ids...\n"
     ]
    }
   ],
   "source": [
    "print(\"converting to token ids...\")\n",
    "word_id,word_id_len=[],[]\n",
    "for doc in processed_docs:\n",
    "    word_ids=[dictionary.token2id[word] for word in doc]\n",
    "    word_id.append(word_ids)\n",
    "    word_id_len.append(len(word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 设置序列长度，为 每条文本的平均单词数+2倍标准差\n",
    "seq_len=np.round( ( np.mean(word_id_len) + 2*np.std(word_id_len) ) ).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222352"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index=X_train.shape[0]\n",
    "X_train=word_id[:index]\n",
    "X_test=word_id[index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156060"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66292"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156060"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[576, 5857, 768, 911, 1347, 975]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#将训练集分成训练集和交叉验证集集\n",
    "num_test = 32000\n",
    "\n",
    "Y_Val=Y_train[:num_test]\n",
    "X_Val=X_train[:num_test]\n",
    "\n",
    "X_train=X_train[num_test:]\n",
    "Y_train=Y_train[num_test:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[   0,    0,    0, ...,    0, 8030,   60],\n",
       "        [   0,    0,    0, ..., 6464, 6465, 6466],\n",
       "        [   0,    0,    0, ...,    0,  178, 3279],\n",
       "        ..., \n",
       "        [   0,    0,    0, ...,    0, 5468,  243],\n",
       "        [   0,    0,    0, ...,    0, 8905, 6035],\n",
       "        [   0,    0,    0, ...,    0,    0, 9053]], dtype=int32),\n",
       " array([[ 0.,  1.,  0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  1.,  0.],\n",
       "        ..., \n",
       "        [ 0.,  1.,  0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  1.,  0.,  0.]]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#padding all text to same size\n",
    "X_Train_encodedPadded_words = sequence.pad_sequences(X_train, maxlen=seq_len)\n",
    "X_Val_encodedPadded_words = sequence.pad_sequences(X_Val, maxlen=seq_len)\n",
    "X_test_encodedPadded_words = sequence.pad_sequences(X_test, maxlen=seq_len)\n",
    "\n",
    "# One Hot Encoding\n",
    "Y_train = keras.utils.to_categorical(Y_train, 5)\n",
    "Y_Val   = keras.utils.to_categorical(Y_Val, 5)\n",
    "\n",
    "#shuffling the traing Set\n",
    "shuffle_2(X_Train_encodedPadded_words,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features are ['PhraseId' 'SentenceId' 'Phrase' 'Sentiment']\n",
      "-------After extracting a validation set of 32000--------\n",
      "============================== Training data shapes ==============================\n",
      "X_train.shape is  (124060,)\n",
      "Y_train.shape is  (124060, 5)\n",
      "============================== Validation data shapes ==============================\n",
      "X_Val.shape is  (32000,)\n",
      "Y_Val.shape is  (32000, 5)\n",
      "============================== Test data shape ==============================\n",
      "X_test.shape is  (66292,)\n"
     ]
    }
   ],
   "source": [
    "print('features are',feature_names)\n",
    "print(\"-------After extracting a validation set of \"+str(num_test)+\"--------\")\n",
    "print ('============================== Training data shapes ==============================')\n",
    "print ('X_train.shape is ', np.array(X_train).shape)\n",
    "print ('Y_train.shape is ',np.array(Y_train).shape)\n",
    "print ('============================== Validation data shapes ==============================')\n",
    "print ('X_Val.shape is ', np.array(X_Val).shape)\n",
    "print ('Y_Val.shape is ',np.array(Y_Val).shape)\n",
    "print ('============================== Test data shape ==============================')\n",
    "print ('X_test.shape is ', np.array(X_test).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== After padding all text to same size of 12 ==============================\n",
      "============================== Training data shapes ==============================\n",
      "X_Train_encodedPadded_words.shape is  (124060, 12)\n",
      "Y_train.shape is  (124060, 5)\n",
      "============================== Validation data shapes ==============================\n",
      "X_Val_encodedPadded_words.shape is  (32000, 12)\n",
      "Y_Val.shape is  (32000, 5)\n",
      "============================== Test data shape ==============================\n",
      "X_test_encodedPadded_words.shape is  (66292, 12)\n"
     ]
    }
   ],
   "source": [
    "print ('============================== After padding all text to same size of '+ str(seq_len)+' ==============================')\n",
    "print ('============================== Training data shapes ==============================')\n",
    "print ('X_Train_encodedPadded_words.shape is ', X_Train_encodedPadded_words.shape)\n",
    "print ('Y_train.shape is ',Y_train.shape)\n",
    "print ('============================== Validation data shapes ==============================')\n",
    "print ('X_Val_encodedPadded_words.shape is ', X_Val_encodedPadded_words.shape)\n",
    "print ('Y_Val.shape is ',Y_Val.shape)\n",
    "print ('============================== Test data shape ==============================')\n",
    "print ('X_test_encodedPadded_words.shape is ', X_test_encodedPadded_words.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 32)          440288    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 128)               82432     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 539,877\n",
      "Trainable params: 539,877\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzz/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(128, activation=\"relu\", kernel_constraint=<keras.con...)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "model=Sequential()\n",
    "model.add(Embedding(dictionary_size,32))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(128, activation='relu',W_constraint=maxnorm(1)))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate=0.0001\n",
    "epochs = 2\n",
    "batch_size = 256 #32\n",
    "sgd = SGD(lr=learning_rate, nesterov=True, momentum=0.7, decay=1e-4)\n",
    "Nadam = keras.optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Nadam, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tensorboard=keras.callbacks.TensorBoard(log_dir='./logs/log_25',histogram_freq=0,write_graph=True,write_images=False) #该回调函数是一个可视化的展示器\n",
    "checkpointer=ModelCheckpoint(filepath='./weights/weights_25.hdf5',verbose=1,save_best_only=True,monitor='val_loss') #该回调函数将在每个epoch后保存模型到filepath\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=0, verbose=1, mode='auto', cooldown=0, min_lr=1e-6) #当评价指标不在提升时，减少学习率\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=6, verbose=1) #当监测值不再改善时，该回调函数将中止训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================== Training =========================================\n",
      "Train on 124060 samples, validate on 32000 samples\n",
      "Epoch 1/2\n",
      "123648/124060 [============================>.] - ETA: 0s - loss: 1.0551 - acc: 0.5765Epoch 00000: val_loss improved from inf to 0.93920, saving model to ./weights/weights_25.hdf5\n",
      "124060/124060 [==============================] - 15s - loss: 1.0547 - acc: 0.5766 - val_loss: 0.9392 - val_acc: 0.6248\n",
      "Epoch 2/2\n",
      "123648/124060 [============================>.] - ETA: 0s - loss: 0.8488 - acc: 0.6512Epoch 00001: val_loss improved from 0.93920 to 0.92638, saving model to ./weights/weights_25.hdf5\n",
      "124060/124060 [==============================] - 15s - loss: 0.8488 - acc: 0.6512 - val_loss: 0.9264 - val_acc: 0.6355\n"
     ]
    }
   ],
   "source": [
    "print (\"=============================== Training =========================================\")\n",
    "history  = model.fit(X_Train_encodedPadded_words, Y_train, epochs = epochs, batch_size=batch_size, verbose=1,\n",
    "                    validation_data=(X_Val_encodedPadded_words, Y_Val), callbacks=[tensorboard, reduce_lr,checkpointer,earlyStopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================== Score =========================================\n",
      "Accuracy: 63.55%\n"
     ]
    }
   ],
   "source": [
    "print (\"=============================== Score =========================================\")\n",
    "scores = model.evaluate(X_Val_encodedPadded_words, Y_Val, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================== Predicting =========================================\n",
      "65792/66292 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "print (\"=============================== Predicting =========================================\")\n",
    "f = open('Submission.csv', 'w')\n",
    "f.write('PhraseId,Sentiment\\n')\n",
    "predicted_classes = model.predict_classes(X_test_encodedPadded_words, batch_size=batch_size, verbose=1)\n",
    "for i in range(0,X_test_PhraseID.shape[0]):\n",
    "    # pred =np.argmax(predictions[i])\n",
    "    f.write(str(X_test_PhraseID[i])+\",\"+str(predicted_classes[i])+'\\n')\n",
    "    # print predictions[i],\"=>\",pred\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tensorboard --logdir=./logs/log_25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
