{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec训练词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import nltk.data\n",
    "#nltk.download()\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(name, nrows=None):\n",
    "    datasets = {\n",
    "        'unlabeled_train': 'unlabeledTrainData.tsv',\n",
    "        'labeled_train': 'labeledTrainData.tsv',\n",
    "        'test': 'testData.tsv'\n",
    "    }\n",
    "    if name not in datasets:\n",
    "        raise ValueError(name)\n",
    "    data_file = os.path.join('..', 'data', datasets[name])\n",
    "    df = pd.read_csv(data_file, sep='\\t', escapechar='\\\\', nrows=nrows)\n",
    "    print('Number of reviews: {}'.format(len(df)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读入无标签数据\n",
    "用于训练生成word2vec词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews: 50000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9999_0</td>\n",
       "      <td>Watching Time Chasers, it obvious that it was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45057_0</td>\n",
       "      <td>I saw this film about 20 years ago and remembe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15561_0</td>\n",
       "      <td>Minor Spoilers&lt;br /&gt;&lt;br /&gt;In New York, Joan Ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7161_0</td>\n",
       "      <td>I went to see this film with a great deal of e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43971_0</td>\n",
       "      <td>Yes, I agree with everyone on this site this m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                             review\n",
       "0   9999_0  Watching Time Chasers, it obvious that it was ...\n",
       "1  45057_0  I saw this film about 20 years ago and remembe...\n",
       "2  15561_0  Minor Spoilers<br /><br />In New York, Joan Ba...\n",
       "3   7161_0  I went to see this film with a great deal of e...\n",
       "4  43971_0  Yes, I agree with everyone on this site this m..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_dataset('unlabeled_train')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 和第一个ipython notebook一样做数据的预处理\n",
    "稍稍有一点不一样的是，我们留了个候选，可以去除停用词，也可以不去除停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eng_stopwords = set(stopwords.words('english'))\n",
    "eng_stopwords = {}.fromkeys([ line.rstrip() for line in open('../stopwords.txt')])\n",
    "\n",
    "def clean_text(text, remove_stopwords=False):\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text() # 去除HTML标签符\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text) # 去除标点符号\n",
    "    words = text.lower().split() # 分词\n",
    "    if remove_stopwords:\n",
    "        words = [w for w in words if w not in eng_stopwords] # 去除停用词\n",
    "    return words\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def print_call_counts(f):\n",
    "    n = 0\n",
    "    def wrapped(*args, **kwargs):\n",
    "        nonlocal n\n",
    "        n += 1\n",
    "        if n % 1000 == 1:\n",
    "            print('method {} called {} times'.format(f.__name__, n))\n",
    "        return f(*args, **kwargs)\n",
    "    return wrapped\n",
    "\n",
    "@print_call_counts\n",
    "def split_sentences(review):\n",
    "    raw_sentences = tokenizer.tokenize(review.strip()) # 分句\n",
    "    sentences = [clean_text(s) for s in raw_sentences if s]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method split_sentences called 1 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzz/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "/home/zzz/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.archive.org/details/LovefromaStranger\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/zzz/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:219: UserWarning: \"b'..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method split_sentences called 1001 times\n",
      "method split_sentences called 2001 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzz/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method split_sentences called 3001 times\n",
      "method split_sentences called 4001 times\n",
      "method split_sentences called 5001 times\n",
      "method split_sentences called 6001 times\n",
      "method split_sentences called 7001 times\n",
      "method split_sentences called 8001 times\n",
      "method split_sentences called 9001 times\n",
      "method split_sentences called 10001 times\n",
      "method split_sentences called 11001 times\n",
      "method split_sentences called 12001 times\n",
      "method split_sentences called 13001 times\n",
      "method split_sentences called 14001 times\n",
      "method split_sentences called 15001 times\n",
      "method split_sentences called 16001 times\n",
      "method split_sentences called 17001 times\n",
      "method split_sentences called 18001 times\n",
      "method split_sentences called 19001 times\n",
      "method split_sentences called 20001 times\n",
      "method split_sentences called 21001 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzz/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method split_sentences called 22001 times\n",
      "method split_sentences called 23001 times\n",
      "method split_sentences called 24001 times\n",
      "method split_sentences called 25001 times\n",
      "method split_sentences called 26001 times\n",
      "method split_sentences called 27001 times\n",
      "method split_sentences called 28001 times\n",
      "method split_sentences called 29001 times\n",
      "method split_sentences called 30001 times\n",
      "method split_sentences called 31001 times\n",
      "method split_sentences called 32001 times\n",
      "method split_sentences called 33001 times\n",
      "method split_sentences called 34001 times\n",
      "method split_sentences called 35001 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzz/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method split_sentences called 36001 times\n",
      "method split_sentences called 37001 times\n",
      "method split_sentences called 38001 times\n",
      "method split_sentences called 39001 times\n",
      "method split_sentences called 40001 times\n",
      "method split_sentences called 41001 times\n",
      "method split_sentences called 42001 times\n",
      "method split_sentences called 43001 times\n",
      "method split_sentences called 44001 times\n",
      "method split_sentences called 45001 times\n",
      "method split_sentences called 46001 times\n",
      "method split_sentences called 47001 times\n",
      "method split_sentences called 48001 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzz/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method split_sentences called 49001 times\n",
      "CPU times: user 8min 6s, sys: 18.5 s, total: 8min 24s\n",
      "Wall time: 8min 23s\n",
      "50000 reviews -> 537851 sentences\n"
     ]
    }
   ],
   "source": [
    "%time sentences = sum(df.review.apply(split_sentences), [])\n",
    "print('{} reviews -> {} sentences'.format(len(df), len(sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "537851"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用gensim训练词嵌入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 设定词向量训练的参数\n",
    "num_features = 300    # Word vector dimensionality\n",
    "min_word_count = 40   # Minimum word count\n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size\n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "model_name = '{}features_{}minwords_{}context.model'.format(num_features, min_word_count, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-29 11:22:34,568 : INFO : collecting all words and their counts\n",
      "2017-12-29 11:22:34,569 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-29 11:22:34,630 : INFO : PROGRESS: at sentence #10000, processed 225072 words, keeping 17237 word types\n",
      "2017-12-29 11:22:34,696 : INFO : PROGRESS: at sentence #20000, processed 443536 words, keeping 24570 word types\n",
      "2017-12-29 11:22:34,762 : INFO : PROGRESS: at sentence #30000, processed 666343 words, keeping 29785 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-29 11:22:34,829 : INFO : PROGRESS: at sentence #40000, processed 886903 words, keeping 33939 word types\n",
      "2017-12-29 11:22:34,897 : INFO : PROGRESS: at sentence #50000, processed 1103863 words, keeping 37503 word types\n",
      "2017-12-29 11:22:34,966 : INFO : PROGRESS: at sentence #60000, processed 1327231 words, keeping 40738 word types\n",
      "2017-12-29 11:22:35,036 : INFO : PROGRESS: at sentence #70000, processed 1550828 words, keeping 43603 word types\n",
      "2017-12-29 11:22:35,105 : INFO : PROGRESS: at sentence #80000, processed 1772824 words, keeping 46155 word types\n",
      "2017-12-29 11:22:35,172 : INFO : PROGRESS: at sentence #90000, processed 1987492 words, keeping 48328 word types\n",
      "2017-12-29 11:22:35,242 : INFO : PROGRESS: at sentence #100000, processed 2210772 words, keeping 50551 word types\n",
      "2017-12-29 11:22:35,311 : INFO : PROGRESS: at sentence #110000, processed 2435496 words, keeping 52762 word types\n",
      "2017-12-29 11:22:35,380 : INFO : PROGRESS: at sentence #120000, processed 2658449 words, keeping 54893 word types\n",
      "2017-12-29 11:22:35,449 : INFO : PROGRESS: at sentence #130000, processed 2877962 words, keeping 56598 word types\n",
      "2017-12-29 11:22:35,517 : INFO : PROGRESS: at sentence #140000, processed 3098235 words, keeping 58352 word types\n",
      "2017-12-29 11:22:35,585 : INFO : PROGRESS: at sentence #150000, processed 3315370 words, keeping 60013 word types\n",
      "2017-12-29 11:22:35,654 : INFO : PROGRESS: at sentence #160000, processed 3536039 words, keeping 61691 word types\n",
      "2017-12-29 11:22:35,723 : INFO : PROGRESS: at sentence #170000, processed 3758385 words, keeping 63292 word types\n",
      "2017-12-29 11:22:35,792 : INFO : PROGRESS: at sentence #180000, processed 3979413 words, keeping 64846 word types\n",
      "2017-12-29 11:22:35,862 : INFO : PROGRESS: at sentence #190000, processed 4203546 words, keeping 66403 word types\n",
      "2017-12-29 11:22:35,933 : INFO : PROGRESS: at sentence #200000, processed 4429481 words, keeping 67924 word types\n",
      "2017-12-29 11:22:36,003 : INFO : PROGRESS: at sentence #210000, processed 4652920 words, keeping 69248 word types\n",
      "2017-12-29 11:22:36,072 : INFO : PROGRESS: at sentence #220000, processed 4870835 words, keeping 70567 word types\n",
      "2017-12-29 11:22:36,142 : INFO : PROGRESS: at sentence #230000, processed 5093104 words, keeping 71912 word types\n",
      "2017-12-29 11:22:36,212 : INFO : PROGRESS: at sentence #240000, processed 5311435 words, keeping 73234 word types\n",
      "2017-12-29 11:22:36,281 : INFO : PROGRESS: at sentence #250000, processed 5532195 words, keeping 74486 word types\n",
      "2017-12-29 11:22:36,351 : INFO : PROGRESS: at sentence #260000, processed 5751629 words, keeping 75693 word types\n",
      "2017-12-29 11:22:36,421 : INFO : PROGRESS: at sentence #270000, processed 5973493 words, keeping 76829 word types\n",
      "2017-12-29 11:22:36,491 : INFO : PROGRESS: at sentence #280000, processed 6191000 words, keeping 77953 word types\n",
      "2017-12-29 11:22:36,562 : INFO : PROGRESS: at sentence #290000, processed 6415980 words, keeping 79135 word types\n",
      "2017-12-29 11:22:36,632 : INFO : PROGRESS: at sentence #300000, processed 6634726 words, keeping 80229 word types\n",
      "2017-12-29 11:22:36,703 : INFO : PROGRESS: at sentence #310000, processed 6857605 words, keeping 81308 word types\n",
      "2017-12-29 11:22:36,773 : INFO : PROGRESS: at sentence #320000, processed 7077123 words, keeping 82421 word types\n",
      "2017-12-29 11:22:36,844 : INFO : PROGRESS: at sentence #330000, processed 7298667 words, keeping 83509 word types\n",
      "2017-12-29 11:22:36,914 : INFO : PROGRESS: at sentence #340000, processed 7516302 words, keeping 84445 word types\n",
      "2017-12-29 11:22:36,984 : INFO : PROGRESS: at sentence #350000, processed 7735101 words, keeping 85566 word types\n",
      "2017-12-29 11:22:37,055 : INFO : PROGRESS: at sentence #360000, processed 7956254 words, keeping 86544 word types\n",
      "2017-12-29 11:22:37,129 : INFO : PROGRESS: at sentence #370000, processed 8177574 words, keeping 87489 word types\n",
      "2017-12-29 11:22:37,200 : INFO : PROGRESS: at sentence #380000, processed 8395550 words, keeping 88515 word types\n",
      "2017-12-29 11:22:37,273 : INFO : PROGRESS: at sentence #390000, processed 8616518 words, keeping 89500 word types\n",
      "2017-12-29 11:22:37,345 : INFO : PROGRESS: at sentence #400000, processed 8835616 words, keeping 90470 word types\n",
      "2017-12-29 11:22:37,417 : INFO : PROGRESS: at sentence #410000, processed 9055384 words, keeping 91344 word types\n",
      "2017-12-29 11:22:37,490 : INFO : PROGRESS: at sentence #420000, processed 9276296 words, keeping 92245 word types\n",
      "2017-12-29 11:22:37,561 : INFO : PROGRESS: at sentence #430000, processed 9494459 words, keeping 93176 word types\n",
      "2017-12-29 11:22:37,635 : INFO : PROGRESS: at sentence #440000, processed 9719312 words, keeping 94119 word types\n",
      "2017-12-29 11:22:37,707 : INFO : PROGRESS: at sentence #450000, processed 9936915 words, keeping 94980 word types\n",
      "2017-12-29 11:22:37,781 : INFO : PROGRESS: at sentence #460000, processed 10160053 words, keeping 95781 word types\n",
      "2017-12-29 11:22:37,853 : INFO : PROGRESS: at sentence #470000, processed 10380740 words, keeping 96637 word types\n",
      "2017-12-29 11:22:37,925 : INFO : PROGRESS: at sentence #480000, processed 10599172 words, keeping 97471 word types\n",
      "2017-12-29 11:22:37,997 : INFO : PROGRESS: at sentence #490000, processed 10816559 words, keeping 98279 word types\n",
      "2017-12-29 11:22:38,068 : INFO : PROGRESS: at sentence #500000, processed 11032175 words, keeping 99064 word types\n",
      "2017-12-29 11:22:38,141 : INFO : PROGRESS: at sentence #510000, processed 11254508 words, keeping 99930 word types\n",
      "2017-12-29 11:22:38,217 : INFO : PROGRESS: at sentence #520000, processed 11481357 words, keeping 100836 word types\n",
      "2017-12-29 11:22:38,290 : INFO : PROGRESS: at sentence #530000, processed 11704018 words, keeping 101618 word types\n",
      "2017-12-29 11:22:38,348 : INFO : collected 102304 word types from a corpus of 11877522 raw words and 537851 sentences\n",
      "2017-12-29 11:22:38,349 : INFO : Loading a fresh vocabulary\n",
      "2017-12-29 11:22:38,444 : INFO : min_count=40 retains 13056 unique words (12% of original 102304, drops 89248)\n",
      "2017-12-29 11:22:38,445 : INFO : min_count=40 leaves 11401019 word corpus (95% of original 11877522, drops 476503)\n",
      "2017-12-29 11:22:38,506 : INFO : deleting the raw counts dictionary of 102304 items\n",
      "2017-12-29 11:22:38,511 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2017-12-29 11:22:38,512 : INFO : downsampling leaves estimated 8394665 word corpus (73.6% of prior 11401019)\n",
      "2017-12-29 11:22:38,513 : INFO : estimated required memory for 13056 words and 300 dimensions: 37862400 bytes\n",
      "2017-12-29 11:22:38,576 : INFO : resetting layer weights\n",
      "2017-12-29 11:22:38,883 : INFO : training model with 4 workers on 13056 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-12-29 11:22:39,930 : INFO : PROGRESS: at 0.35% examples, 142443 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:22:40,933 : INFO : PROGRESS: at 0.75% examples, 155368 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:22:41,948 : INFO : PROGRESS: at 1.16% examples, 159370 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:22:42,960 : INFO : PROGRESS: at 1.56% examples, 161378 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:22:43,970 : INFO : PROGRESS: at 1.96% examples, 162609 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:22:44,983 : INFO : PROGRESS: at 2.37% examples, 163359 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:22:46,005 : INFO : PROGRESS: at 2.76% examples, 163725 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:22:47,010 : INFO : PROGRESS: at 3.17% examples, 164328 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:22:48,022 : INFO : PROGRESS: at 3.58% examples, 164653 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:22:49,046 : INFO : PROGRESS: at 3.99% examples, 165376 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:22:50,051 : INFO : PROGRESS: at 4.39% examples, 165656 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:22:51,059 : INFO : PROGRESS: at 4.79% examples, 165869 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:22:52,070 : INFO : PROGRESS: at 5.20% examples, 166003 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:22:53,073 : INFO : PROGRESS: at 5.61% examples, 166188 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:22:54,093 : INFO : PROGRESS: at 6.01% examples, 166178 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-29 11:22:55,106 : INFO : PROGRESS: at 6.41% examples, 166232 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:22:56,122 : INFO : PROGRESS: at 6.81% examples, 166246 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:22:57,137 : INFO : PROGRESS: at 7.21% examples, 166253 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:22:58,156 : INFO : PROGRESS: at 7.61% examples, 166264 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:22:59,172 : INFO : PROGRESS: at 8.03% examples, 166617 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:00,182 : INFO : PROGRESS: at 8.43% examples, 166675 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:01,184 : INFO : PROGRESS: at 8.82% examples, 166460 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:02,202 : INFO : PROGRESS: at 9.23% examples, 166451 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:03,218 : INFO : PROGRESS: at 9.63% examples, 166459 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:04,235 : INFO : PROGRESS: at 10.03% examples, 166467 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:05,246 : INFO : PROGRESS: at 10.44% examples, 166506 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:06,251 : INFO : PROGRESS: at 10.84% examples, 166572 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:07,259 : INFO : PROGRESS: at 11.24% examples, 166631 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:08,262 : INFO : PROGRESS: at 11.64% examples, 166689 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:09,272 : INFO : PROGRESS: at 12.05% examples, 166735 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:10,277 : INFO : PROGRESS: at 12.45% examples, 166787 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:11,281 : INFO : PROGRESS: at 12.87% examples, 166854 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:12,282 : INFO : PROGRESS: at 13.27% examples, 166910 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:13,291 : INFO : PROGRESS: at 13.67% examples, 166950 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:14,359 : INFO : PROGRESS: at 14.10% examples, 166885 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:15,366 : INFO : PROGRESS: at 14.50% examples, 166929 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:16,378 : INFO : PROGRESS: at 14.90% examples, 166936 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:17,388 : INFO : PROGRESS: at 15.31% examples, 166956 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:18,397 : INFO : PROGRESS: at 15.71% examples, 166985 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:19,414 : INFO : PROGRESS: at 16.12% examples, 166964 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:20,442 : INFO : PROGRESS: at 16.52% examples, 166908 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:21,453 : INFO : PROGRESS: at 16.92% examples, 166919 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:22,479 : INFO : PROGRESS: at 17.33% examples, 166876 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:23,487 : INFO : PROGRESS: at 17.73% examples, 166907 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:24,500 : INFO : PROGRESS: at 18.14% examples, 166903 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:25,502 : INFO : PROGRESS: at 18.55% examples, 166950 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:26,515 : INFO : PROGRESS: at 18.96% examples, 166951 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:27,530 : INFO : PROGRESS: at 19.35% examples, 166944 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:28,542 : INFO : PROGRESS: at 19.75% examples, 166957 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:29,546 : INFO : PROGRESS: at 20.15% examples, 166983 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:30,559 : INFO : PROGRESS: at 20.55% examples, 166979 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:31,573 : INFO : PROGRESS: at 20.96% examples, 166985 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:32,586 : INFO : PROGRESS: at 21.36% examples, 166993 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:33,588 : INFO : PROGRESS: at 21.76% examples, 167031 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:34,599 : INFO : PROGRESS: at 22.16% examples, 167035 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:35,610 : INFO : PROGRESS: at 22.56% examples, 167040 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:36,620 : INFO : PROGRESS: at 22.97% examples, 167050 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:37,630 : INFO : PROGRESS: at 23.38% examples, 167071 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:38,645 : INFO : PROGRESS: at 23.78% examples, 167059 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:39,657 : INFO : PROGRESS: at 24.18% examples, 167063 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:40,672 : INFO : PROGRESS: at 24.58% examples, 167064 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:41,694 : INFO : PROGRESS: at 24.98% examples, 167037 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:42,704 : INFO : PROGRESS: at 25.39% examples, 167045 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:43,707 : INFO : PROGRESS: at 25.80% examples, 167069 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:44,719 : INFO : PROGRESS: at 26.19% examples, 167063 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:45,731 : INFO : PROGRESS: at 26.60% examples, 167069 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:46,733 : INFO : PROGRESS: at 27.00% examples, 167096 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:47,745 : INFO : PROGRESS: at 27.39% examples, 167098 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:48,753 : INFO : PROGRESS: at 27.79% examples, 167117 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:49,771 : INFO : PROGRESS: at 28.20% examples, 167101 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:50,779 : INFO : PROGRESS: at 28.60% examples, 167111 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:51,790 : INFO : PROGRESS: at 29.01% examples, 167119 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:52,846 : INFO : PROGRESS: at 29.43% examples, 167116 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:53,855 : INFO : PROGRESS: at 29.83% examples, 167124 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-29 11:23:54,869 : INFO : PROGRESS: at 30.23% examples, 167124 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:55,887 : INFO : PROGRESS: at 30.64% examples, 167118 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:56,894 : INFO : PROGRESS: at 31.05% examples, 167130 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:57,907 : INFO : PROGRESS: at 31.44% examples, 167137 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:58,913 : INFO : PROGRESS: at 31.85% examples, 167149 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:23:59,915 : INFO : PROGRESS: at 32.25% examples, 167173 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:00,929 : INFO : PROGRESS: at 32.66% examples, 167173 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:01,939 : INFO : PROGRESS: at 33.06% examples, 167174 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:02,943 : INFO : PROGRESS: at 33.46% examples, 167194 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:03,951 : INFO : PROGRESS: at 33.87% examples, 167203 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:04,955 : INFO : PROGRESS: at 34.28% examples, 167220 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:05,959 : INFO : PROGRESS: at 34.69% examples, 167234 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:06,962 : INFO : PROGRESS: at 35.09% examples, 167257 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:07,972 : INFO : PROGRESS: at 35.49% examples, 167259 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:08,973 : INFO : PROGRESS: at 35.90% examples, 167279 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:09,981 : INFO : PROGRESS: at 36.30% examples, 167284 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:11,062 : INFO : PROGRESS: at 36.73% examples, 167234 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:12,069 : INFO : PROGRESS: at 37.12% examples, 167246 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:13,085 : INFO : PROGRESS: at 37.52% examples, 167239 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:14,091 : INFO : PROGRESS: at 37.93% examples, 167251 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:15,094 : INFO : PROGRESS: at 38.35% examples, 167268 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-29 11:24:16,106 : INFO : PROGRESS: at 38.76% examples, 167270 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:17,124 : INFO : PROGRESS: at 39.15% examples, 167264 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:18,139 : INFO : PROGRESS: at 39.55% examples, 167264 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:19,167 : INFO : PROGRESS: at 39.97% examples, 167305 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:20,175 : INFO : PROGRESS: at 40.36% examples, 167309 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:21,186 : INFO : PROGRESS: at 40.77% examples, 167308 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:22,201 : INFO : PROGRESS: at 41.17% examples, 167305 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:23,203 : INFO : PROGRESS: at 41.58% examples, 167322 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:24,216 : INFO : PROGRESS: at 41.98% examples, 167320 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:25,255 : INFO : PROGRESS: at 42.40% examples, 167341 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:26,263 : INFO : PROGRESS: at 42.80% examples, 167346 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:27,281 : INFO : PROGRESS: at 43.20% examples, 167339 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:28,305 : INFO : PROGRESS: at 43.61% examples, 167319 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:29,318 : INFO : PROGRESS: at 44.01% examples, 167315 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:30,329 : INFO : PROGRESS: at 44.41% examples, 167315 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:31,333 : INFO : PROGRESS: at 44.81% examples, 167329 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:32,341 : INFO : PROGRESS: at 45.21% examples, 167335 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:33,354 : INFO : PROGRESS: at 45.63% examples, 167330 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:34,355 : INFO : PROGRESS: at 46.03% examples, 167346 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:35,357 : INFO : PROGRESS: at 46.43% examples, 167358 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:36,361 : INFO : PROGRESS: at 46.83% examples, 167367 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:37,365 : INFO : PROGRESS: at 47.22% examples, 167374 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:38,367 : INFO : PROGRESS: at 47.62% examples, 167391 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:39,369 : INFO : PROGRESS: at 48.03% examples, 167402 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:40,378 : INFO : PROGRESS: at 48.41% examples, 167348 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:41,385 : INFO : PROGRESS: at 48.82% examples, 167357 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:42,398 : INFO : PROGRESS: at 49.23% examples, 167355 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:43,423 : INFO : PROGRESS: at 49.63% examples, 167338 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-29 11:24:44,429 : INFO : PROGRESS: at 50.03% examples, 167345 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:45,439 : INFO : PROGRESS: at 50.44% examples, 167349 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:46,453 : INFO : PROGRESS: at 50.84% examples, 167346 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:47,462 : INFO : PROGRESS: at 51.24% examples, 167351 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:48,472 : INFO : PROGRESS: at 51.64% examples, 167351 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:49,475 : INFO : PROGRESS: at 52.05% examples, 167363 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:50,480 : INFO : PROGRESS: at 52.45% examples, 167369 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:51,482 : INFO : PROGRESS: at 52.87% examples, 167380 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:52,487 : INFO : PROGRESS: at 53.27% examples, 167387 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:53,488 : INFO : PROGRESS: at 53.67% examples, 167400 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:54,542 : INFO : PROGRESS: at 54.10% examples, 167398 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:55,548 : INFO : PROGRESS: at 54.50% examples, 167407 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:56,554 : INFO : PROGRESS: at 54.90% examples, 167414 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:57,564 : INFO : PROGRESS: at 55.31% examples, 167418 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:58,569 : INFO : PROGRESS: at 55.71% examples, 167428 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:24:59,578 : INFO : PROGRESS: at 56.12% examples, 167429 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:00,586 : INFO : PROGRESS: at 56.52% examples, 167433 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:01,592 : INFO : PROGRESS: at 56.92% examples, 167437 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:02,673 : INFO : PROGRESS: at 57.34% examples, 167404 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:03,681 : INFO : PROGRESS: at 57.75% examples, 167410 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:04,692 : INFO : PROGRESS: at 58.16% examples, 167410 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:05,729 : INFO : PROGRESS: at 58.57% examples, 167382 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:06,739 : INFO : PROGRESS: at 58.97% examples, 167383 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:07,795 : INFO : PROGRESS: at 59.36% examples, 167333 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:08,809 : INFO : PROGRESS: at 59.76% examples, 167333 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-29 11:25:09,871 : INFO : PROGRESS: at 60.18% examples, 167322 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:10,900 : INFO : PROGRESS: at 60.60% examples, 167347 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:11,926 : INFO : PROGRESS: at 61.01% examples, 167334 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:12,939 : INFO : PROGRESS: at 61.39% examples, 167286 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:13,965 : INFO : PROGRESS: at 61.80% examples, 167270 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:15,018 : INFO : PROGRESS: at 62.19% examples, 167224 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:16,038 : INFO : PROGRESS: at 62.60% examples, 167215 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:17,066 : INFO : PROGRESS: at 63.00% examples, 167201 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:18,077 : INFO : PROGRESS: at 63.41% examples, 167204 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:19,089 : INFO : PROGRESS: at 63.81% examples, 167201 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:20,139 : INFO : PROGRESS: at 64.21% examples, 167164 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:21,149 : INFO : PROGRESS: at 64.59% examples, 167125 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:22,159 : INFO : PROGRESS: at 64.99% examples, 167127 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:23,236 : INFO : PROGRESS: at 65.42% examples, 167106 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:24,282 : INFO : PROGRESS: at 65.83% examples, 167073 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:25,305 : INFO : PROGRESS: at 66.23% examples, 167060 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:26,340 : INFO : PROGRESS: at 66.63% examples, 167038 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:27,353 : INFO : PROGRESS: at 67.03% examples, 167037 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:28,377 : INFO : PROGRESS: at 67.42% examples, 167026 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:29,377 : INFO : PROGRESS: at 67.82% examples, 167040 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:30,385 : INFO : PROGRESS: at 68.21% examples, 167004 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:31,421 : INFO : PROGRESS: at 68.61% examples, 166981 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:32,423 : INFO : PROGRESS: at 69.03% examples, 166993 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:33,442 : INFO : PROGRESS: at 69.43% examples, 166987 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:34,478 : INFO : PROGRESS: at 69.83% examples, 166965 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:35,536 : INFO : PROGRESS: at 70.23% examples, 166923 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:36,553 : INFO : PROGRESS: at 70.64% examples, 166923 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-29 11:25:37,564 : INFO : PROGRESS: at 71.05% examples, 166924 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:38,590 : INFO : PROGRESS: at 71.43% examples, 166877 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:39,632 : INFO : PROGRESS: at 71.83% examples, 166854 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:40,663 : INFO : PROGRESS: at 72.23% examples, 166837 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:41,670 : INFO : PROGRESS: at 72.64% examples, 166845 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:42,701 : INFO : PROGRESS: at 73.05% examples, 166828 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:43,739 : INFO : PROGRESS: at 73.45% examples, 166807 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:44,749 : INFO : PROGRESS: at 73.86% examples, 166812 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:45,774 : INFO : PROGRESS: at 74.27% examples, 166803 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:46,804 : INFO : PROGRESS: at 74.67% examples, 166789 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:47,868 : INFO : PROGRESS: at 75.08% examples, 166748 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:48,902 : INFO : PROGRESS: at 75.47% examples, 166731 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:49,941 : INFO : PROGRESS: at 75.88% examples, 166713 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:50,959 : INFO : PROGRESS: at 76.29% examples, 166709 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:51,978 : INFO : PROGRESS: at 76.69% examples, 166706 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:53,005 : INFO : PROGRESS: at 77.09% examples, 166698 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:54,027 : INFO : PROGRESS: at 77.49% examples, 166692 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:55,066 : INFO : PROGRESS: at 77.90% examples, 166673 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:56,089 : INFO : PROGRESS: at 78.31% examples, 166667 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:57,091 : INFO : PROGRESS: at 78.73% examples, 166678 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:58,092 : INFO : PROGRESS: at 79.10% examples, 166655 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:25:59,100 : INFO : PROGRESS: at 79.48% examples, 166627 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:26:00,121 : INFO : PROGRESS: at 79.88% examples, 166622 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:26:01,166 : INFO : PROGRESS: at 80.28% examples, 166597 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:26:02,198 : INFO : PROGRESS: at 80.70% examples, 166617 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:26:03,213 : INFO : PROGRESS: at 81.10% examples, 166618 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:26:04,216 : INFO : PROGRESS: at 81.51% examples, 166628 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:26:05,219 : INFO : PROGRESS: at 81.90% examples, 166605 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:26:06,253 : INFO : PROGRESS: at 82.30% examples, 166588 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:26:07,303 : INFO : PROGRESS: at 82.70% examples, 166562 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:26:08,333 : INFO : PROGRESS: at 83.10% examples, 166551 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:26:09,341 : INFO : PROGRESS: at 83.51% examples, 166558 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:26:10,440 : INFO : PROGRESS: at 83.92% examples, 166524 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:26:11,463 : INFO : PROGRESS: at 84.33% examples, 166518 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:26:12,474 : INFO : PROGRESS: at 84.73% examples, 166524 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:26:13,484 : INFO : PROGRESS: at 85.15% examples, 166562 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:26:14,486 : INFO : PROGRESS: at 85.54% examples, 166540 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:26:15,521 : INFO : PROGRESS: at 85.94% examples, 166525 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:26:16,536 : INFO : PROGRESS: at 86.33% examples, 166492 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:26:17,567 : INFO : PROGRESS: at 86.73% examples, 166482 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:26:18,651 : INFO : PROGRESS: at 87.15% examples, 166461 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:26:19,660 : INFO : PROGRESS: at 87.54% examples, 166469 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:26:20,684 : INFO : PROGRESS: at 87.94% examples, 166462 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:26:21,710 : INFO : PROGRESS: at 88.34% examples, 166454 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:26:22,710 : INFO : PROGRESS: at 88.75% examples, 166469 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:26:23,755 : INFO : PROGRESS: at 89.16% examples, 166447 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:26:24,800 : INFO : PROGRESS: at 89.56% examples, 166427 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:26:25,821 : INFO : PROGRESS: at 89.97% examples, 166425 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:26:26,828 : INFO : PROGRESS: at 90.37% examples, 166433 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:26:27,869 : INFO : PROGRESS: at 90.75% examples, 166386 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:26:28,876 : INFO : PROGRESS: at 91.16% examples, 166394 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:26:29,883 : INFO : PROGRESS: at 91.56% examples, 166402 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:26:30,886 : INFO : PROGRESS: at 91.95% examples, 166384 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:26:31,924 : INFO : PROGRESS: at 92.35% examples, 166370 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:26:32,952 : INFO : PROGRESS: at 92.77% examples, 166362 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:26:33,967 : INFO : PROGRESS: at 93.16% examples, 166364 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:26:34,981 : INFO : PROGRESS: at 93.57% examples, 166366 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:26:35,995 : INFO : PROGRESS: at 93.98% examples, 166369 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:26:37,009 : INFO : PROGRESS: at 94.37% examples, 166342 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:26:38,038 : INFO : PROGRESS: at 94.77% examples, 166335 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:26:39,136 : INFO : PROGRESS: at 95.19% examples, 166309 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:26:40,161 : INFO : PROGRESS: at 95.60% examples, 166305 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:26:41,197 : INFO : PROGRESS: at 96.00% examples, 166294 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:26:42,215 : INFO : PROGRESS: at 96.40% examples, 166293 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:26:43,284 : INFO : PROGRESS: at 96.81% examples, 166258 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:26:44,298 : INFO : PROGRESS: at 97.21% examples, 166260 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:26:45,341 : INFO : PROGRESS: at 97.61% examples, 166245 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:26:46,367 : INFO : PROGRESS: at 98.02% examples, 166239 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:26:47,405 : INFO : PROGRESS: at 98.43% examples, 166228 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:26:48,446 : INFO : PROGRESS: at 98.84% examples, 166213 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:26:49,455 : INFO : PROGRESS: at 99.24% examples, 166220 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:26:50,484 : INFO : PROGRESS: at 99.61% examples, 166185 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-29 11:26:51,310 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-29 11:26:51,353 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-29 11:26:51,355 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-29 11:26:51,376 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-29 11:26:51,377 : INFO : training on 59387610 raw words (41972583 effective words) took 252.5s, 166237 effective words/s\n",
      "2017-12-29 11:26:51,379 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-12-29 11:26:51,573 : INFO : saving Word2Vec object under ../models/300features_40minwords_10context.model, separately None\n",
      "2017-12-29 11:26:51,574 : INFO : not storing attribute syn0norm\n",
      "2017-12-29 11:26:51,576 : INFO : not storing attribute cum_table\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-29 11:26:52,028 : INFO : saved ../models/300features_40minwords_10context.model\n"
     ]
    }
   ],
   "source": [
    "print('Training model...')\n",
    "model = Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model.save(os.path.join('..', 'models', model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 看看训练的词向量结果如何"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kitchen\n",
      "berlin\n"
     ]
    }
   ],
   "source": [
    "print(model.doesnt_match(\"man woman child kitchen\".split()))\n",
    "print(model.doesnt_match('france england germany berlin'.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.651971697807312),\n",
       " ('lady', 0.6159939765930176),\n",
       " ('lad', 0.5849895477294922),\n",
       " ('guy', 0.5439465641975403),\n",
       " ('soldier', 0.5393285751342773),\n",
       " ('chap', 0.536977231502533),\n",
       " ('person', 0.5364387035369873),\n",
       " ('monk', 0.5182623863220215),\n",
       " ('gentleman', 0.5176940560340881),\n",
       " ('boy', 0.5167154669761658)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('belle', 1.2401440143585205),\n",
       " ('blonde', 1.2320035696029663),\n",
       " ('catherine', 1.2289553880691528),\n",
       " ('regina', 1.2160236835479736),\n",
       " ('angela', 1.2042949199676514),\n",
       " ('virgin', 1.2029750347137451),\n",
       " ('marlene', 1.1997132301330566),\n",
       " ('mistress', 1.1830108165740967),\n",
       " ('goddess', 1.1782617568969727),\n",
       " ('madame', 1.1761484146118164)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"queen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('terrible', 1.567256212234497),\n",
       " ('horrible', 1.5290379524230957),\n",
       " ('dreadful', 1.4438960552215576),\n",
       " ('abysmal', 1.4089943170547485),\n",
       " ('embarrassing', 1.2835288047790527),\n",
       " ('crappy', 1.2247406244277954),\n",
       " ('lame', 1.220945119857788),\n",
       " ('ridiculous', 1.1674494743347168),\n",
       " ('incomprehensible', 1.1456104516983032),\n",
       " ('unbelievable', 1.1332685947418213)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"awful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
